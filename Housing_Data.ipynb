{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Housing Data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rsfwalters/NEU-OB-MLP/blob/main/Housing_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Housing Data\n",
        "\n",
        "In this notebook, we load data about housing prices together with other factors which influence housing prices.  The main goal is to predict housing prices."
      ],
      "metadata": {
        "id": "y4Xf4NqMR_JZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load Data\n"
      ],
      "metadata": {
        "id": "ALyZyqknSLQn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start by loading the data.  This data contains housing prices in california and some factors which may be useful in predicting housing prices.  See details about this dataset: https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset\n",
        "\n",
        "You can load other datasets from OpenML: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_openml.html\n",
        "\n"
      ],
      "metadata": {
        "id": "qLeRatxskbeN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JfjzqwaHRNMy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "pd.set_option('display.expand_frame_repr', False)  # Show all columns when printing a pandas dataframe\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "# Load the data once as a Pandas DataFrame, so we can inspect the features\n",
        "housing_frame = fetch_california_housing(as_frame=True).frame"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's take a look at our data.  We first print some example rows and then a summary of statistics about the data."
      ],
      "metadata": {
        "id": "wR07ytYBwgxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(housing_frame)"
      ],
      "metadata": {
        "id": "zYZf4uRTbpT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(housing_frame.describe())"
      ],
      "metadata": {
        "id": "UK_Hqm7jcCyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**Question**] What do the various columns and rows of this table mean?\n",
        "\n",
        "[**Question**] Which of thses other columns (\"features\") will be most useful in predicting housing price? "
      ],
      "metadata": {
        "id": "e7oWQpnHwvPI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's visualize the data using a histogram plot.  We can quickly see which values occur more frequently.  Here we view the \"Median Income\" and \"House Age\" feature.  "
      ],
      "metadata": {
        "id": "5zX9vBOGxLdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from plotly.subplots import make_subplots\n",
        "\n",
        "fig = make_subplots(rows=2, cols=1)\n",
        "\n",
        "for idx, col_name in enumerate([\"MedInc\", \"HouseAge\"]):\n",
        "    fig.add_trace(go.Histogram(x=housing_frame[col_name], name=col_name), row=idx+1, col=1)\n",
        "    fig.update_xaxes(title_text=col_name, row=idx+1, col=1)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "KLo-pSZ-i7e3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**Question**] How would you describe the difference between the Median Income and House Age values in terms of the peaks or spread.  What do you think explains this?\n",
        "\n",
        "[**Question**] Try changing the code to look at other columns (variables) in the data"
      ],
      "metadata": {
        "id": "cEUSXyLVxen4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now look at our target output variable (median house price in units of $100k)"
      ],
      "metadata": {
        "id": "ngaBOYK3kryV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = go.Figure()\n",
        "fig.add_trace(go.Histogram(x=housing_frame[\"MedHouseVal\"]))\n",
        "fig.update_layout(\n",
        "    title=\"Histogram of label variable (median house price) \",\n",
        "    xaxis_title=\"Median House Price (x $100K)\",\n",
        "    yaxis_title=\"Counts\",\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "YgzC-KDIXche"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [**Optional**] Additional Inputs\n",
        "\n",
        "These commented blocks show other possible inputs we can use for predictions.\n",
        "\n",
        "[**Question**] Try uncommenting them and using them as input variables!"
      ],
      "metadata": {
        "id": "qKZoudTy60H7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having looked at our data, we see that some features are highly skewed, and are spread on very different ranges.\n",
        "A common normalization method is to describe each value in terms of the mean and standard deviation for the whole group.\n",
        "We can get this representation by subtracting the mean and dividing by the standard deviation.\n",
        "This will help bring different features into a similar scale - Notice how the X-axis of our histogram changes!"
      ],
      "metadata": {
        "id": "j5BLVt1NzupQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mean_medinc = np.mean(housing_frame[\"MedInc\"])\n",
        "# stdev_medinc = np.std(housing_frame[\"MedInc\"])\n",
        "# normalized_medinc = (housing_frame[\"MedInc\"] - mean_medinc) / stdev_medinc\n",
        "\n",
        "# float_age = np.array(housing_frame[\"HouseAge\"]).astype(np.float32)\n",
        "# mean_houseage = np.mean(float_age)\n",
        "# stdev_houseage = np.std(float_age)\n",
        "# normalized_houseage = (float_age - mean_houseage) / stdev_houseage\n",
        "\n",
        "# # We can inspect how this changed the distribution of these two input variables.\n",
        "\n",
        "# from plotly.subplots import make_subplots\n",
        "# fig = make_subplots(rows=2, cols=1)\n",
        "# for idx, (values, name) in enumerate([(normalized_medinc, \"Normalized MedInc\"), (normalized_houseage, \"Normalized HouseAge\")]):\n",
        "#     fig.add_trace(go.Histogram(x=values, name=name), row=idx+1, col=1)\n",
        "#     fig.update_xaxes(title_text=name, row=idx+1, col=1)\n",
        "# fig.show()\n",
        "\n",
        "# normalized_medinc = np.expand_dims(normalized_medinc, 1)\n",
        "# normalized_houseage = np.expand_dims(normalized_houseage, 1)"
      ],
      "metadata": {
        "id": "kjTbPFQLl0b-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**Question**] Try uncommenting this block and using `normalized_medinc` or `normalized_houseage` as an input"
      ],
      "metadata": {
        "id": "Sc6xQa2F0Klv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also try adjusting the \"constrast\" - or moving values around within this range to more evenly spread them out."
      ],
      "metadata": {
        "id": "Mspq4Wkw0Tfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.preprocessing import QuantileTransformer\n",
        "# scaler = QuantileTransformer(output_distribution=\"uniform\")   # EXERCISE: Try adjusting the distribution to \"normal\" instead!\n",
        "# q_medinc = scaler.fit_transform(np.expand_dims(housing_frame[\"MedInc\"], -1)).squeeze()\n",
        "# q_houseage = scaler.fit_transform(np.expand_dims(housing_frame[\"HouseAge\"], -1)).squeeze()\n",
        "\n",
        "# from plotly.subplots import make_subplots\n",
        "# fig = make_subplots(rows=2, cols=1)\n",
        "# for idx, (values, name) in enumerate([(q_medinc, \"Normalized MedInc\"), (q_houseage, \"Normalized HouseAge\")]):\n",
        "#     fig.add_trace(go.Histogram(x=values, name=name), row=idx+1, col=1)\n",
        "#     fig.update_xaxes(title_text=name, row=idx+1, col=1)\n",
        "# fig.show()\n",
        "\n",
        "# q_medinc = np.expand_dims(q_medinc, 1)\n",
        "# q_houseage = np.expand_dims(q_houseage, 1)"
      ],
      "metadata": {
        "id": "WtiFDThjm1Tl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**Question**] Try using `q_medinc` and `q_houseage` as additional inputs to the model."
      ],
      "metadata": {
        "id": "dXBPIf1N0VxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can truncate the range of values used, to suppress the effect of outliers. This way, the model will see a more narrow range of values. This approach is simpler than using a percentile transformation, but many items may end up having the minimum or maximum value."
      ],
      "metadata": {
        "id": "eSbHjiFk0hbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # trunc_medinc = np.clip(housing_frame[\"MedInc\"], 0, 8).squeeze()\n",
        " # trunc_houseage = np.clip(housing_frame[\"HouseAge\"], 0, 35).squeeze()\n",
        " # from plotly.subplots import make_subplots\n",
        " # fig = make_subplots(rows=2, cols=1)\n",
        " # for idx, (values, name) in enumerate([(trunc_medinc, \"Truncated MedInc\"), (trunc_houseage, \"Truncated HouseAge\")]):\n",
        " #    fig.add_trace(go.Histogram(x=values, name=name), row=idx+1, col=1)\n",
        " #    fig.update_xaxes(title_text=name, row=idx+1, col=1)\n",
        " # fig.show()\n",
        "\n",
        " # trunc_medinc = np.expand_dims(trunc_medinc, 1)\n",
        " # trunc_houseage = np.expand_dims(trunc_houseage, 1)"
      ],
      "metadata": {
        "id": "-qN_1OY9qBjl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**Question**] Try using `trunc_medinc` and `trunc_houseage` as additional inputs to the model.\n",
        "\n",
        "[**Question**] `clip(x,0,8)` replaces values smaller than 0 with 0 and values larger than 8 with 8. Try changing the range of values in clip to reasonable values.\n"
      ],
      "metadata": {
        "id": "auGMff7B0tRo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input Variable Selection\n",
        "\n",
        "Now, let's select a subset of features to use as inputs to the model.\n",
        "We'll select columns from the pandas dataframe, and convert them into a numpy array.\n",
        "We'll choose median house value (\"MedHouseVal\") as the target variable.\n",
        "We can also engineer additional features based on our intuition about this problem."
      ],
      "metadata": {
        "id": "5zW6C2Lvk-gk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ppl_per_bedroom = housing_frame[\"AveOccup\"] / housing_frame[\"AveBedrms\"]\n",
        "ppl_per_bedroom = np.expand_dims(ppl_per_bedroom, 1)  # To easily combine with our other cols, we'll reshape to (Items, 1)\n",
        "# ppl_per_room = housing_frame[\"AveOccup\"] / housing_frame[\"AveRooms\"]\n",
        "# ppl_per_room = np.expand_dims(ppl_per_room, 1)\n",
        "\n",
        "selected_columns = np.array(housing_frame[[\"MedInc\", \"HouseAge\"]])\n",
        "print(\"Before adding a feature:\", selected_columns.shape)\n",
        "data = np.concatenate([selected_columns, ppl_per_bedroom], axis=1)  #Here is where we select our inputs\n",
        "print(\"After adding a feature:\", data.shape)\n",
        "labels = np.array(housing_frame[\"MedHouseVal\"]) \n",
        "print(\"Labels:\", labels.shape)"
      ],
      "metadata": {
        "id": "JlmVi7YAb77I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f893e54-6681-4a47-cd98-4ec1783c4321"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before adding a feature: (20640, 2)\n",
            "After adding a feature: (20640, 3)\n",
            "Labels: (20640,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**Question**] Try adding an input variable such as `ppl_per_room` or a variable from the \"Additonal Inputs\" section to the model where `selected_columns, ppl_per_bedrop` is."
      ],
      "metadata": {
        "id": "WsxIqGz109JI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the data into a train, validation, and test set.\n",
        "We'll try to extract as much information as we can from the train set.\n",
        "Periodically, we can check the validation set to see how we're doing (like a practice quiz)\n",
        "Only once, at the very end, we check our performance on the test set."
      ],
      "metadata": {
        "id": "vhBls_8Pk5kV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_items = data.shape[0]\n",
        "\n",
        "test_fraction = 0.1\n",
        "val_fraction = 0.1\n",
        "shuffled_indices = np.random.permutation(N_items)   # EXERCISE: Try not shuffling the data\n",
        "N_test = int(test_fraction * N_items)\n",
        "N_val = int(val_fraction * N_items)\n",
        "\n",
        "shuffled_data = data[shuffled_indices]\n",
        "shuffled_labels = labels[shuffled_indices]\n",
        "test_data, test_labels = shuffled_data[:N_test], shuffled_labels[:N_test]\n",
        "val_data, val_labels = shuffled_data[N_test:N_test+N_val], shuffled_labels[N_test:N_test+N_val]\n",
        "train_data, train_labels = shuffled_data[N_test+N_val:], shuffled_labels[N_test+N_val:]\n",
        "\n",
        "print(\"Training set:\", train_data.shape, train_labels.shape)\n",
        "print(\"Validation set:\", val_data.shape, val_labels.shape)\n",
        "print(\"Test set:\", test_data.shape, test_labels.shape)"
      ],
      "metadata": {
        "id": "IKrrUmcNg0V_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1404f17-b965-4235-bf60-3ea9533df948"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set: (16512, 3) (16512,)\n",
            "Validation set: (2064, 3) (2064,)\n",
            "Test set: (2064, 3) (2064,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**Question**] Try adjusting the fraction of data used for training, validation, and test.\n",
        "\n",
        "[**Question**] Try not shuffling the data.  To do this you can just set the `shuffled_data` and `shuffled_labels` to be equal to `data` and `labels`.\n"
      ],
      "metadata": {
        "id": "lPUvpJaP2-NE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use 32-bit floating point values, which should be plenty of precision for our purposes.\n",
        "Note that the data type of our data must match the data type of our model layers later on."
      ],
      "metadata": {
        "id": "pwI9ZulAlEiU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, train_labels = train_data.astype(\"float32\"), train_labels.astype(\"float32\")\n",
        "val_data, val_labels = val_data.astype(\"float32\"), val_labels.astype(\"float32\")\n",
        "test_data, test_labels = test_data.astype(\"float32\"), test_labels.astype(\"float32\")"
      ],
      "metadata": {
        "id": "5FTgU8DqkG33"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the data for use in pytorch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    TensorDataset(torch.as_tensor(train_data), torch.as_tensor(train_labels)),\n",
        "    # EXERCISE: Try varying the training batch size\n",
        "    batch_size=32,     # How many items should we grab for each gradient descent step\n",
        "    pin_memory=True,   # Helps transfer data to/from GPU faster\n",
        "    shuffle=True,      # In each epoch, we shuffle the training data\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    TensorDataset(torch.as_tensor(val_data), torch.as_tensor(val_labels)),\n",
        "    batch_size=32,     # How many items should we grab for each gradient descent step\n",
        "    pin_memory=True,   # Helps transfer data to/from GPU faster\n",
        "    shuffle=False,     \n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    TensorDataset(torch.as_tensor(test_data), torch.as_tensor(test_labels)),\n",
        "    batch_size=32,     # How many items should we grab for each gradient descent step\n",
        "    pin_memory=True,   # Helps transfer data to/from GPU faster\n",
        "    shuffle=False,     \n",
        ")"
      ],
      "metadata": {
        "id": "vgpN7IhaZRqd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Define Model"
      ],
      "metadata": {
        "id": "lSHehL2xSZWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torch.optim import Adam, SGD\n",
        "from torch.optim.lr_scheduler import MultiplicativeLR\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(3, 32)  # NOTE - we must match our input dimension with the number of input variables used\n",
        "        self.layer2 = nn.Linear(32, 32)\n",
        "        self.layer3 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Here x is a batch of input data\n",
        "        h1 = self.layer1(x)\n",
        "        h1 = F.relu(h1)\n",
        "        h2 = self.layer2(h1)\n",
        "        h2 = F.relu(h2)\n",
        "        # h2 = F.relu(h2) + h1\n",
        "        h3 = self.layer3(h2)\n",
        "        return h3\n",
        "\n",
        "model = MyModel()\n",
        "model = model.to(\"cuda\")  # Send the model to the GPU\n",
        "\n",
        "# We will use the optimizer to adjust the model parameters after predicting each batch\n",
        "optim = Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.999), weight_decay=1e-3)   \n",
        "# After each epoch, we can reduce the learning rate. \n",
        "sched = MultiplicativeLR(optim, lr_lambda=lambda epoch: 0.95)"
      ],
      "metadata": {
        "id": "-NKmv-WpRn9_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**Question**] Try varying the width of the hidden layers.  Careful, the output size of each layer must match the input size of next layer.\n",
        "\n",
        "[**Question**] Try adding or removing a layer.  Note, you must define a new layer inside `__init__` and then use it inside `forward`.\n",
        "\n",
        "[**Question**] Try add a \"skip connection.\"  For example, replace the current `h2 = F.relu(h2)` line with `h2 = F.relu(h2) + h1`\n",
        "\n",
        "[**Question**]  Try applying an activation at the end to restrict the output to a reasonable range. For example, try using F.sigmoid (https://pytorch.org/docs/stable/generated/torch.nn.functional.sigmoid.html#torch.nn.functional.sigmoid)  and then multiplying by a number.\n",
        "\n",
        "[**Question**]  Try using a different optimizer, or varying parameters like learning rate, momentum, and weight decay\n",
        "`optim = SGD(model.parameters(), lr=1e-3, momentum=1e-5, weight_decay=1e-5)`\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fssbWhuH3eit"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Train and Evaluate\n",
        "\n",
        "Note - if you adjust the training procedure, be sure to re-initialize the model by re-running that cell above. Otherwise, your model would not be starting \"from scratch\"; it would start training from the point where it left off.\n"
      ],
      "metadata": {
        "id": "Hny7V65-Tv7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm, trange"
      ],
      "metadata": {
        "id": "5iBH9bd5fO6M"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll keep note of our training loss at every batch, and our validation loss at every epoch\n",
        "train_loss_tracking, val_loss_tracking = [], [] \n",
        "test_preds = []\n",
        "epochs = 2\n",
        "\n",
        "for epoch in trange(epochs, desc=\"epochs\", leave=True, position=0):\n",
        "    # Perform an epoch of training\n",
        "    model.train()  # Set the model into training mode. This affects some layer behavior, such as dropout and batchnorm\n",
        "    for batch_data, batch_labels in tqdm(train_dataloader, desc=\"batches\", leave=False, position=1):\n",
        "        batch_data = batch_data.to(\"cuda\")\n",
        "        batch_labels = batch_labels.to(\"cuda\")\n",
        "\n",
        "        predictions = model(batch_data).squeeze()\n",
        "        loss = F.mse_loss(predictions, batch_labels)\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        train_loss_tracking.append(loss.item())\n",
        "\n",
        "    # Check our performance on the validation set\n",
        "    with torch.no_grad():\n",
        "        avg_val_loss = 0.0\n",
        "        model.eval()\n",
        "        for batch_data, batch_labels in tqdm(val_dataloader, desc=\"batches\", leave=False, position=1):\n",
        "            batch_data = batch_data.to(\"cuda\")\n",
        "            batch_labels = batch_labels.to(\"cuda\")\n",
        "\n",
        "            predictions = model(batch_data).squeeze()\n",
        "            avg_val_loss += F.mse_loss(predictions, batch_labels, reduction=\"sum\")  # Sum here, since we'll average ourselves\n",
        "        avg_val_loss /= len(val_dataloader.dataset)    \n",
        "        val_loss_tracking.append(avg_val_loss.item())\n",
        "        print(len(val_loss_tracking))\n",
        "    sched.step()  # Adjust our learning rate after every epoch\n",
        "\n",
        "\n",
        "# Finally at the very end, check our performance on the test set\n",
        "with torch.no_grad():\n",
        "    model.eval()   # Set the model into evaluation mode.\n",
        "    avg_test_loss = 0.0\n",
        "    for batch_data, batch_labels in tqdm(test_dataloader):\n",
        "        batch_data = batch_data.to(\"cuda\")\n",
        "        batch_labels = batch_labels.to(\"cuda\")\n",
        "        predictions = model(batch_data).squeeze()\n",
        "        test_preds.extend(list(predictions.cpu().numpy()))\n",
        "        avg_test_loss += F.mse_loss(predictions, batch_labels, reduction=\"sum\")\n",
        "    avg_test_loss /= len(test_dataloader.dataset)"
      ],
      "metadata": {
        "id": "Gr4x9tc-Ttp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the model is trained, we see how we did.  The next line will print the average test error.  For example, an error of 0.8 means our house price predictions are wrong by 80K on average.  Not too bad."
      ],
      "metadata": {
        "id": "QqLj6DLHmBim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Average test loss:\", round(avg_test_loss.item(), 3))"
      ],
      "metadata": {
        "id": "lGXft21Cmpz2",
        "outputId": "3dade9a0-4faa-4b70-afa2-b9e3348452fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average test loss: 0.649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also plot the true versus predicted values"
      ],
      "metadata": {
        "id": "yOW8ljfSmSwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=test_labels, y=test_preds, mode=\"markers\", showlegend=False))\n",
        "fig.add_trace(go.Scatter(x=[0,5], y=[0,5], mode='lines', showlegend=False))\n",
        "fig.update_traces(patch={\"line\": { \"dash\": \"dot\"}})\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Compare true and predicted values\",\n",
        "    xaxis_title=\"True price\",\n",
        "    yaxis_title=\"Predicted price\",\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "0G6KQggwyRlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**Question**] In the plot above, the blue dots show predictions of your model.  A perfectly accurate model would place all predictions on the red line (where predicted price equals true price).  What can you say about the predictions of your model? Can you come up with an explanation for why this is happening?"
      ],
      "metadata": {
        "id": "H73NmQCthlZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "N = 20\n",
        "\n",
        "mean_x3 = np.mean(ppl_per_bedroom).astype(np.float32)\n",
        "\n",
        "dummy_data = []\n",
        "for x1 in np.linspace(np.min(test_data[:, 0]), np.max(test_data[:, 0]), N, dtype=np.float32):\n",
        "    for x2 in np.linspace(np.min(test_data[:, 1]), np.max(test_data[:, 1]), N,dtype=np.float32):\n",
        "            dummy_data.append((x1, x2, mean_x3))\n",
        "\n",
        "dummy_data = np.array(dummy_data)\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    dummy_preds = model(torch.as_tensor(dummy_data).to(\"cuda\")).cpu().numpy().squeeze()\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Contour(x=dummy_data[:, 0], z=dummy_data[:, 1], y=dummy_preds,\n",
        "                         contours=dict(start=0, end=50, size=10)))\n",
        "fig.update_layout(\n",
        "    title=\"Predicted Home Value (Yellow is higher predicted price, Purple is lower)\",\n",
        "    xaxis_title=\"House Age\",\n",
        "    yaxis_title=\"Med Income\",\n",
        "    \n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "c5ovwqTFy8Db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above, we visualize the impact of inputs on the prediction.  Here purple represents lower predicted home values and yellow high values.  The x-axis shows the impact of house age.  The y-axis shows the impact of median income.  \n",
        "\n",
        "[**Question**] Which are positively or negatively correlated with the prediction? Do you agree with the model's understanding?\n",
        "\n",
        "[**Question**] What do you think the horizontal patterns indicate?"
      ],
      "metadata": {
        "id": "0T00o75j4zIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from plotly.subplots import make_subplots\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.update_layout(\n",
        "    title=\"Training and Validation loss\",\n",
        "    xaxis_title=\"Batch number\",\n",
        "    yaxis_title=\"Loss value\",\n",
        ")\n",
        "fig.add_trace(go.Scatter(x=np.arange(len(train_loss_tracking)), y=train_loss_tracking, name=\"Training\"))\n",
        "\n",
        "# Each item in the validation loss list was measured after an epoch.\n",
        "# Each epoch consists of multiple batches\n",
        "batches_per_epoch = len(train_dataloader)\n",
        "x = np.arange(1, len(val_loss_tracking)+1) * batches_per_epoch\n",
        "fig.add_trace(go.Scatter(x=x, y=val_loss_tracking, name=\"Validation\"))\n",
        "fig.update_yaxes(range=[0, 13])\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "N8dK2gkmgVpb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}